{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Car Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéØ The goal of this challenge is to prepare a dataset and apply some feature selection techniques that you have learned so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üöó We are dealing with a dataset about cars and we would like to predict whether a car is expensive or cheap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Data visualisation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Checking whether a numerical feature has a normal distribution or not\n",
    "from statsmodels.graphics.gofplots import qqplot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Cars_dataset.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Go ahead and load the CSV into a dataframe called `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(url)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ÑπÔ∏è The description of the dataset is available [here](https://wagon-public-datasets.s3.amazonaws.com/Machine%20Learning%20Datasets/ML_Cars_dataset_description.txt). Make sure to refer to it throughout the exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (1) Duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Remove the duplicates from the dataset if there are any. ‚ùì\n",
    "\n",
    "*Overwite the dataframe `df`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "df = df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (2) Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Find the missing values and impute them either with `strategy = \"most frequent\"` (categorical variables) or `strategy = \"median\"` (numerical variables) ‚ùì\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "numerical_columns = df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy=\"median\")\n",
    "df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `carwidth`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary> üí° <i>Hint</i> </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>carwidth</code> has multiple representations for missing values. Some are <code>np.nans</code>, some are  <code>*</code>. Once located, they can be imputed by the median value, since missing values make up less than 30% of the data.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "df['carwidth'] = df['carwidth'].replace('*', np.nan)\n",
    "\n",
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "numerical_columns = df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy=\"median\")\n",
    "df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `enginelocation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° <i>Hint</i> </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è Considering that <code>enginelocation</code> is a categorical feature, and that the vast majority of the category is <code>front</code>, impute with the most frequent.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "numerical_columns = df.select_dtypes(exclude=['object']).columns\n",
    "\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "df[categorical_columns] = categorical_imputer.fit_transform(df[categorical_columns])\n",
    "\n",
    "numerical_imputer = SimpleImputer(strategy=\"median\")\n",
    "df[numerical_columns] = numerical_imputer.fit_transform(df[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/reecepalmer/.pyenv/versions/3.10.6/envs/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/reecepalmer/Code/RPalmr/05-ML/02-Prepare-the-dataset/data-car-prices/tests\n",
      "plugins: asyncio-0.19.0, dash-2.14.0, typeguard-2.13.3, anyio-3.6.2, hydra-core-1.3.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 2 items\n",
      "\n",
      "test_missing_values.py::TestMissing_values::test_carwidth \u001b[32mPASSED\u001b[0m\u001b[32m         [ 50%]\u001b[0m\n",
      "test_missing_values.py::TestMissing_values::test_engine_location \u001b[32mPASSED\u001b[0m\u001b[32m  [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m2 passed\u001b[0m\u001b[32m in 0.27s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/missing_values.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed missing_values step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('missing_values',\n",
    "                         dataset = df)\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (3) Scaling the numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 191 entries, 0 to 204\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   aspiration      191 non-null    object \n",
      " 1   enginelocation  191 non-null    object \n",
      " 2   carwidth        191 non-null    object \n",
      " 3   curbweight      191 non-null    float64\n",
      " 4   enginetype      191 non-null    object \n",
      " 5   cylindernumber  191 non-null    object \n",
      " 6   stroke          191 non-null    float64\n",
      " 7   peakrpm         191 non-null    float64\n",
      " 8   price           191 non-null    object \n",
      "dtypes: float64(3), object(6)\n",
      "memory usage: 14.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# As a reminder, some information about the dataframe\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['curbweight', 'stroke', 'peakrpm'], dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And here are the numerical features of the dataset we need to scale\n",
    "numerical_features = df.select_dtypes(exclude=['object']).columns\n",
    "numerical_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: Scaling the numerical features** ‚ùì\n",
    "\n",
    "Investigate the numerical features for outliers and distribution, and apply the solutions below accordingly:\n",
    "- Robust Scaler\n",
    "- Standard Scaler\n",
    "\n",
    "Replace the original columns with the transformed values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `peakrpm` , `carwidth` , & `stroke`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° <i>Hint</i> </summary>\n",
    "\n",
    "    \n",
    "‚ÑπÔ∏è <code>peakrpm</code>, <code>carwidth</code>, & <code>stroke</code> have normal distributions but also some outliers. Hence, it is advisable to use `RobustScaler()`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "columns_to_scale_robust = ['peakrpm', 'carwidth', 'stroke']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "robust_scaler = RobustScaler()\n",
    "\n",
    "df[columns_to_scale_robust] = robust_scaler.fit_transform(df[columns_to_scale_robust])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `curbweight`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° <i>Hint</i> </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>curbweight</code> has a normal distribution and no outliers. It can be Standard Scaled.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "columns_to_scale_standard = ['curbweight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "standard_scaler = StandardScaler()\n",
    "\n",
    "df[columns_to_scale_standard] = standard_scaler.fit_transform(df[columns_to_scale_standard])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/reecepalmer/.pyenv/versions/3.10.6/envs/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/reecepalmer/Code/RPalmr/05-ML/02-Prepare-the-dataset/data-car-prices/tests\n",
      "plugins: asyncio-0.19.0, dash-2.14.0, typeguard-2.13.3, anyio-3.6.2, hydra-core-1.3.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "test_scaling.py::TestScaling::test_carwidth \u001b[32mPASSED\u001b[0m\u001b[32m                       [ 25%]\u001b[0m\n",
      "test_scaling.py::TestScaling::test_curbweight \u001b[32mPASSED\u001b[0m\u001b[32m                     [ 50%]\u001b[0m\n",
      "test_scaling.py::TestScaling::test_peakrpm \u001b[32mPASSED\u001b[0m\u001b[32m                        [ 75%]\u001b[0m\n",
      "test_scaling.py::TestScaling::test_stroke \u001b[32mPASSED\u001b[0m\u001b[32m                         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.30s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/scaling.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed scaling step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('scaling',\n",
    "                         dataset = df\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (4) Encoding the categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question: encoding the categorical variables** ‚ùì\n",
    "\n",
    "üëá Investigate the features that require encoding, and apply the following techniques accordingly:\n",
    "\n",
    "- One-hot encoding\n",
    "- Manual ordinal encoding\n",
    "\n",
    "In the Dataframe, replace the original features with their encoded version(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `aspiration` & `enginelocation`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° <i>Hint</i> </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>aspiration</code> and <code>enginelocation</code> are binary categorical features.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "binary_mapping = {'std': 0, 'turbo': 1, 'front': 0, 'rear': 1}\n",
    "\n",
    "df['aspiration'] = df['aspiration'].map(binary_mapping)\n",
    "df['enginelocation'] = df['enginelocation'].map(binary_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `enginetype`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° <i>Hint</i> </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>enginetype</code> is a multicategorical feature and must be One hot encoded.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, columns=['enginetype'], prefix=['enginetype'], drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(191, 14)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `cylindernumber`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "\n",
    "‚ÑπÔ∏è <code>cylindernumber</code> is an ordinal feature and must be manually encoded into numeric.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "cylinder_mapping = {'two': 2, 'three': 3, 'four': 4, 'five': 5, 'six': 6, 'eight': 8, 'twelve': 12}\n",
    "\n",
    "df['cylindernumber'] = df['cylindernumber'].map(cylinder_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Now that you've made `cylindernumber` into a numeric feature between 2 and 12, you need to scale it ‚ùì\n",
    "\n",
    "<br/>\n",
    "\n",
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "\n",
    "Look at the current distribution of the `cylindernumber` and ask yourself the following questions:\n",
    "- Does scaling affect a feature's distribution ?\n",
    "- According to the distribution of this feature, what is the most appropriate scaling method?\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "df['cylindernumber'] = minmax_scaler.fit_transform(df[['cylindernumber']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary><i>Here is a screenshot of how your dataframe shoud look like after scaling and encoding</i></summary>\n",
    "    \n",
    "    \n",
    "<img src=\"https://wagon-public-datasets.s3.amazonaws.com/05-Machine-Learning/02-Prepare-the-dataset/car_price_after_scaling_and_encoding.png\">    \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `price`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Encode the target `price`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>üí° Hint </summary>\n",
    "    <br>\n",
    "    ‚ÑπÔ∏è <code>price</code> is the target and must be Label encoded.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "df['price'] = label_encoder.fit_transform(df['price'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/reecepalmer/.pyenv/versions/3.10.6/envs/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/reecepalmer/Code/RPalmr/05-ML/02-Prepare-the-dataset/data-car-prices/tests\n",
      "plugins: asyncio-0.19.0, dash-2.14.0, typeguard-2.13.3, anyio-3.6.2, hydra-core-1.3.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 4 items\n",
      "\n",
      "test_encoding.py::TestEncoding::test_aspiration \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 25%]\u001b[0m\n",
      "test_encoding.py::TestEncoding::test_enginelocation \u001b[32mPASSED\u001b[0m\u001b[32m               [ 50%]\u001b[0m\n",
      "test_encoding.py::TestEncoding::test_enginetype \u001b[32mPASSED\u001b[0m\u001b[32m                   [ 75%]\u001b[0m\n",
      "test_encoding.py::TestEncoding::test_price \u001b[32mPASSED\u001b[0m\u001b[32m                        [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m4 passed\u001b[0m\u001b[32m in 0.28s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/encoding.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed encoding step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('encoding',\n",
    "                         dataset = df)\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (5) Base Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëè The dataset has been preprocessed and is now ready to be fitted to a model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì**Question: a first attempt to evaluate a classification model** ‚ùì\n",
    "\n",
    "Cross-validate a `LogisticRegression` on this preprocessed dataset and save its score under a variable named `base_model_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Score (Accuracy): 0.8639676113360324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "X = df.drop(columns=['price'])\n",
    "y = df['price']\n",
    "\n",
    "model = LogisticRegression()\n",
    "\n",
    "base_model_score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "print(\"Base Model Score (Accuracy):\", base_model_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/reecepalmer/.pyenv/versions/3.10.6/envs/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/reecepalmer/Code/RPalmr/05-ML/02-Prepare-the-dataset/data-car-prices/tests\n",
      "plugins: asyncio-0.19.0, dash-2.14.0, typeguard-2.13.3, anyio-3.6.2, hydra-core-1.3.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_base_model.py::TestBase_model::test_base_model_score \u001b[32mPASSED\u001b[0m\u001b[32m         [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.04s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/base_model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed base_model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('base_model',\n",
    "                         score = base_model_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (6) Feature Selection (with _Permutation Importance_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üë©üèª‚Äçüè´ A powerful way to detect whether a feature is relevant or not to predict a target is to:\n",
    "1. Run a model and score it\n",
    "2. Shuffle this feature, re-run the model and score it\n",
    "    - If the performance significantly dropped, the feature is important and you shoudn't have dropped it\n",
    "    - If the performance didn't decrease a lot, the feature may be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Questions** ‚ùì\n",
    "\n",
    "1. Perform a feature permutation to detect which features bring the least amount of information to the model. \n",
    "2. Remove the weak features from your dataset until you notice model performance dropping substantially\n",
    "3. Using your new set of strong features, cross-validate a new model, and save its score under variable name `strong_model_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Model Score (Accuracy): 0.8639676113360324\n",
      "Strong Model Score (Accuracy): 0.9006747638326585\n"
     ]
    }
   ],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "perm_importance = permutation_importance(model, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "feature_importance_scores = perm_importance.importances_mean\n",
    "\n",
    "feature_importance_dict = {feature: score for feature, score in zip(X.columns, feature_importance_scores)}\n",
    "\n",
    "sorted_features = sorted(feature_importance_dict, key=lambda x: feature_importance_dict[x], reverse=True)\n",
    "\n",
    "base_model_score = cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "print(\"Base Model Score (Accuracy):\", base_model_score)\n",
    "\n",
    "strong_features = []\n",
    "strong_model_score = base_model_score\n",
    "\n",
    "for feature in sorted_features:\n",
    "    strong_features.append(feature)\n",
    "    X_strong = X[strong_features]\n",
    "    model_score = cross_val_score(model, X_strong, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "    if model_score < strong_model_score:\n",
    "        break\n",
    "    strong_model_score = model_score\n",
    "\n",
    "print(\"Strong Model Score (Accuracy):\", strong_model_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß™ **Test your code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "platform darwin -- Python 3.10.6, pytest-7.1.3, pluggy-1.0.0 -- /Users/reecepalmer/.pyenv/versions/3.10.6/envs/lewagon/bin/python3\n",
      "cachedir: .pytest_cache\n",
      "rootdir: /Users/reecepalmer/Code/RPalmr/05-ML/02-Prepare-the-dataset/data-car-prices/tests\n",
      "plugins: asyncio-0.19.0, dash-2.14.0, typeguard-2.13.3, anyio-3.6.2, hydra-core-1.3.2\n",
      "asyncio: mode=strict\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 1 item\n",
      "\n",
      "test_strong_model.py::TestStrong_model::test_strong_model_score \u001b[32mPASSED\u001b[0m\u001b[32m   [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m1 passed\u001b[0m\u001b[32m in 0.05s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "\n",
      "\n",
      "üíØ You can commit your code:\n",
      "\n",
      "\u001b[1;32mgit\u001b[39m add tests/strong_model.pickle\n",
      "\n",
      "\u001b[32mgit\u001b[39m commit -m \u001b[33m'Completed strong_model step'\u001b[39m\n",
      "\n",
      "\u001b[32mgit\u001b[39m push origin master\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nbresult import ChallengeResult\n",
    "\n",
    "result = ChallengeResult('strong_model',\n",
    "                         score = strong_model_score\n",
    ")\n",
    "\n",
    "result.write()\n",
    "print(result.check())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus - Stratifying your data ‚öñÔ∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° As we split our data into training and testing, we need to be mindful of the proportion of categorical variables in our dataset - whether it's the classes of our target `y` or a categorical feature in `X`.\n",
    "\n",
    "Let's have a look at an example üëá\n",
    "\n",
    "‚ùì Split your original `X` and `y` into training and testing data, using sklearn's `train_test_split`; use `random_state=1` and `test_size=0.3` to have comparable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Check the proportion of `price` class `1` cars in your training dataset and testing dataset.\n",
    "\n",
    "> _If you check the proportion of them in the raw `df`, it should be very close to 50/50_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of class 1 cars in training dataset: 0.48026315789473684\n",
      "Proportion of class 1 cars in testing dataset: 0.6153846153846154\n"
     ]
    }
   ],
   "source": [
    "train_class_1_proportion = (y_train == 1).mean()\n",
    "test_class_1_proportion = (y_test == 1).mean()\n",
    "\n",
    "print(\"Proportion of class 1 cars in training dataset:\", train_class_1_proportion)\n",
    "print(\"Proportion of class 1 cars in testing dataset:\", test_class_1_proportion)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should still be pretty close to 50/50 ‚òùÔ∏è \n",
    "\n",
    "***But what if we change the random state?*** \n",
    "\n",
    "‚ùì Loop through random states 1 through 10, each time calculating the share of `price` class `1` cars in the training and testing data. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State 1:\n",
      "Proportion of class 1 cars in training dataset: 0.5037593984962406\n",
      "Proportion of class 1 cars in testing dataset: 0.5172413793103449\n",
      "\n",
      "Random State 2:\n",
      "Proportion of class 1 cars in training dataset: 0.48120300751879697\n",
      "Proportion of class 1 cars in testing dataset: 0.5689655172413793\n",
      "\n",
      "Random State 3:\n",
      "Proportion of class 1 cars in training dataset: 0.5037593984962406\n",
      "Proportion of class 1 cars in testing dataset: 0.5172413793103449\n",
      "\n",
      "Random State 4:\n",
      "Proportion of class 1 cars in training dataset: 0.5338345864661654\n",
      "Proportion of class 1 cars in testing dataset: 0.4482758620689655\n",
      "\n",
      "Random State 5:\n",
      "Proportion of class 1 cars in training dataset: 0.5338345864661654\n",
      "Proportion of class 1 cars in testing dataset: 0.4482758620689655\n",
      "\n",
      "Random State 6:\n",
      "Proportion of class 1 cars in training dataset: 0.49624060150375937\n",
      "Proportion of class 1 cars in testing dataset: 0.5344827586206896\n",
      "\n",
      "Random State 7:\n",
      "Proportion of class 1 cars in training dataset: 0.5338345864661654\n",
      "Proportion of class 1 cars in testing dataset: 0.4482758620689655\n",
      "\n",
      "Random State 8:\n",
      "Proportion of class 1 cars in training dataset: 0.48872180451127817\n",
      "Proportion of class 1 cars in testing dataset: 0.5517241379310345\n",
      "\n",
      "Random State 9:\n",
      "Proportion of class 1 cars in training dataset: 0.5789473684210527\n",
      "Proportion of class 1 cars in testing dataset: 0.3448275862068966\n",
      "\n",
      "Random State 10:\n",
      "Proportion of class 1 cars in training dataset: 0.48872180451127817\n",
      "Proportion of class 1 cars in testing dataset: 0.5517241379310345\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_states = range(1, 11)\n",
    "\n",
    "for random_state in random_states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state)\n",
    "\n",
    "    train_class_1_proportion = (y_train == 1).mean()\n",
    "    test_class_1_proportion = (y_test == 1).mean()\n",
    "\n",
    "    print(f\"Random State {random_state}:\")\n",
    "    print(\"Proportion of class 1 cars in training dataset:\", train_class_1_proportion)\n",
    "    print(\"Proportion of class 1 cars in testing dataset:\", test_class_1_proportion)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will observe that the proportion changes every time, sometimes even quite drastically üò±! This can affect model performance!\n",
    "\n",
    "‚ùì Compare the test score of a logistic regression when trained using `train_test_split(random_state=1)` _vs._ `random_state=9` ‚ùì \n",
    "\n",
    "Remember to fit on training data and score on testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with random_state=1: 0.9482758620689655\n",
      "Test Score with random_state=9: 0.8103448275862069\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Train and test split with random_state=1\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "\n",
    "# Train and test split with random_state=9\n",
    "X_train_9, X_test_9, y_train_9, y_test_9 = train_test_split(X, y, test_size=0.3, random_state=9)\n",
    "\n",
    "# Initialize and fit logistic regression models\n",
    "model_1 = LogisticRegression()\n",
    "model_9 = LogisticRegression()\n",
    "\n",
    "model_1.fit(X_train_1, y_train_1)\n",
    "model_9.fit(X_train_9, y_train_9)\n",
    "\n",
    "# Calculate test scores\n",
    "test_score_1 = model_1.score(X_test_1, y_test_1)\n",
    "test_score_9 = model_9.score(X_test_9, y_test_9)\n",
    "\n",
    "print(\"Test Score with random_state=1:\", test_score_1)\n",
    "print(\"Test Score with random_state=9:\", test_score_9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ You should see a much lower score with `random_state=9` because the proportion of class `1` cars in that test set is 34.5%, quite far from the 57.9% in the training set or even the 50% in the original dataset.\n",
    "\n",
    "This is substantial, as this accidental imbalance in our dataset can not only make model performance worse, but also distort the \"reality\" during training or scoring üßê"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***So how do we fix this issue? How do we keep the same distribution of classes across the train set and the test set? üîß***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üéÅ Luckily, this is taken care of by `cross_validate` in sklearn, when the estimator (a.k.a the model) is a classifier and the target is a class. Check out the documentation of the `cv` parameter in üìö [**sklearn.model_selection.cross_validate**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html).\n",
    "\n",
    "The answer is to use the following:\n",
    "\n",
    ">üìö [**Stratification**](https://scikit-learn.org/stable/modules/cross_validation.html#stratification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratification of the target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üí° We can also use the ***strafification*** technique in a `train_test_split`.\n",
    "\n",
    "‚ùì Run through the same 1 to 10 random state loop again, but this time also ***pass `stratify=y` into the holdout method***. ‚ùì"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random State 1:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 2:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 3:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 4:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 5:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 6:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 7:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 8:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 9:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n",
      "Random State 10:\n",
      "Proportion of class 1 cars in training dataset: 0.5112781954887218\n",
      "Proportion of class 1 cars in testing dataset: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random_states = range(1, 11)\n",
    "\n",
    "for random_state in random_states:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=random_state, stratify=y)\n",
    "\n",
    "    train_class_1_proportion = (y_train == 1).mean()\n",
    "    test_class_1_proportion = (y_test == 1).mean()\n",
    "\n",
    "    print(f\"Random State {random_state}:\")\n",
    "    print(\"Proportion of class 1 cars in training dataset:\", train_class_1_proportion)\n",
    "    print(\"Proportion of class 1 cars in testing dataset:\", test_class_1_proportion)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëÄ Even if the random state is changing, the proportion of classes inside the training and testing data is kept the same as in the original `y`. This is what _stratification_ is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `train_test_split` with the `stratify` parameter, we can also preserve proportions of a feature across training and testing data. This can be extremely important, for example:\n",
    "\n",
    "- preserving proportion of male and female customers in predicting churn üôã‚Äç‚ôÇÔ∏è üôã\n",
    "- preserving the proportion big and small houses in predicting their prices üè† üè∞\n",
    "- preserving distribution of 1-5 review scores (multiclass!) in recommending the next product üõçÔ∏è\n",
    "- etc...\n",
    "\n",
    "For instance, in our dataset, to holdout the same share of `aspiration` feature in both training and testing data, we could simply write `train_test_split(X, y, test_size=0.3, stratify=X.aspiration)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw, **`cross_validate` [can automatically stratify the target](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html#:~:text=For%20int/None%20inputs%2C%20if%20the%20estimator%20is%20a%20classifier%20and%20y%20is%20either%20binary%20or%20multiclass%2C%20StratifiedKFold%20is%20used.), but not the features...** ü§î We need a bit of extra work for that.\n",
    "\n",
    "We need `StratifiedKFold` üî¨\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratification - generalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìö [**StratifiedKFold**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html) allows us to split the data into `K` splits, while stratifying on certain columns (features or target).\n",
    "\n",
    "This way, we can do a manual cross-validation while keeping proportions on the categorical features of interest - let's try it with the binary `aspiration` feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8639676113360324"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# initializing a stratified k-fold that will split the data into 5 folds\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "scores = []\n",
    "\n",
    "# .split() method creates an iterator; 'X.aspiration' is the feature that we stratify by\n",
    "for train_indices, test_indices in skf.split(X, X.aspiration):\n",
    "\n",
    "    # 'train_indices' and 'test_indices' are lists of indices that produce proportional splits\n",
    "    X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "    y_train, y_test = y.iloc[train_indices], y.iloc[test_indices]\n",
    "\n",
    "    # initialize and fit a model\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # append a score to get an average of 5 folds in the end\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "\n",
    "np.array(scores).mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üìñ Some sklearn reads on **stratification**:\n",
    "\n",
    "- [Visualization of how different holdout methods in sklearn work](https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html#sphx-glr-auto-examples-model-selection-plot-cv-indices-py)\n",
    "- [Overall cross-validation and stratification understanding](https://scikit-learn.org/stable/modules/cross_validation.html#stratification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ Congratulations! You have prepared a whole dataset, ran feature selection and even learned about stratification üí™\n",
    "\n",
    "üíæ Don't forget to git add/commit/push your notebook...\n",
    "\n",
    "üöÄ ... and move on to the next challenge!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
